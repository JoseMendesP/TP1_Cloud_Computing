{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede81325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 13:59:54,105 - INFO - Processing #0001 \n",
      "2025-09-04 13:59:54,757 - ERROR - AWS credentials not found!\n",
      "2025-09-04 13:59:55,760 - INFO - Processing #0002 \n",
      "2025-09-04 13:59:56,883 - ERROR - AWS credentials not found!\n",
      "2025-09-04 13:59:57,886 - INFO - Processing #0003 \n",
      "2025-09-04 13:59:58,478 - ERROR - AWS credentials not found!\n",
      "2025-09-04 13:59:59,481 - INFO - Processing #0004 \n",
      "2025-09-04 13:59:59,943 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:00,945 - INFO - Processing #0005 \n",
      "2025-09-04 14:00:01,362 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:02,365 - INFO - Processing #0006 \n",
      "2025-09-04 14:00:03,008 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:04,010 - INFO - Processing #0007 \n",
      "2025-09-04 14:00:04,697 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:05,700 - INFO - Processing #0008 \n",
      "2025-09-04 14:00:06,134 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:07,135 - INFO - Processing #0009 \n",
      "2025-09-04 14:00:07,688 - ERROR - AWS credentials not found!\n",
      "2025-09-04 14:00:08,690 - INFO - Processing #0010 \n",
      "2025-09-04 14:00:09,070 - ERROR - AWS credentials not found!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    141\u001b[39m     collector.collect_data(limit=\u001b[32m100\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    139\u001b[39m bucket_name = \u001b[33m\"\u001b[39m\u001b[33mpokemon-scraper-binks\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m collector = DataCollector(bucket_name)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[43mcollector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mDataCollector.collect_data\u001b[39m\u001b[34m(self, limit)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    134\u001b[39m     logging.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "@dataclass\n",
    "class CreatureData:\n",
    "    index: int\n",
    "    name: str\n",
    "    url: str\n",
    "    types: List[str] = None\n",
    "    image_url: Optional[str] = None\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, base_url: str, delay: float = 1.0):\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def fetch_page(self, url: str) -> BeautifulSoup:\n",
    "        response = self.session.get(url)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    def extract_creature_data(self, element) -> Optional[CreatureData]:\n",
    "        cells = element.find_all(\"td\")\n",
    "        if len(cells) < 3:\n",
    "            return None\n",
    "        index_match = re.search(r\"\\d+\", cells[0].get_text(strip=True))\n",
    "        if not index_match:\n",
    "            return None\n",
    "        link_element = element.find(\"a\", href=True)\n",
    "        if not link_element:\n",
    "            return None\n",
    "        return CreatureData(\n",
    "            index=int(index_match.group()),\n",
    "            name=link_element.get_text(strip=True),\n",
    "            url=urljoin(self.base_url, link_element[\"href\"])\n",
    "        )\n",
    "\n",
    "class S3Uploader:\n",
    "    def __init__(self, bucket_name: str):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.s3 = boto3.client(\"s3\")\n",
    "\n",
    "    def upload_image(self, image_data: bytes, filename: str, prefix: str) -> Optional[str]:\n",
    "        s3_key = f\"{prefix}{filename}\"\n",
    "        try:\n",
    "            ext = os.path.splitext(filename)[-1].lower()\n",
    "            content_type = {\n",
    "                \".jpg\": \"image/jpeg\",\n",
    "                \".jpeg\": \"image/jpeg\",\n",
    "                \".png\": \"image/png\",\n",
    "                \".gif\": \"image/gif\"\n",
    "            }.get(ext, \"image/png\")\n",
    "            self.s3.put_object(\n",
    "                Bucket=self.bucket_name,\n",
    "                Key=s3_key,\n",
    "                Body=image_data,\n",
    "                ACL=\"public-read\",\n",
    "                ContentType=content_type\n",
    "            )\n",
    "            return f\"https://{self.bucket_name}.s3.amazonaws.com/{s3_key}\"\n",
    "        except NoCredentialsError:\n",
    "            logging.error(\"AWS credentials not found!\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to upload {filename} to S3: {e}\")\n",
    "            return None\n",
    "\n",
    "class DataCollector:\n",
    "    def __init__(self, bucket_name: str):\n",
    "        self.base_url = \"https://bulbapedia.bulbagarden.net\"\n",
    "        self.list_url = f\"{self.base_url}/wiki/List_of_Pok%C3%A9mon_by_National_Pok%C3%A9dex_number\"\n",
    "        self.scraper = WebScraper(self.base_url)\n",
    "        self.uploader = S3Uploader(bucket_name)\n",
    "\n",
    "    def find_creature_image_and_types(self, url: str) -> (Optional[str], List[str]):\n",
    "        try:\n",
    "            soup = self.scraper.fetch_page(url)\n",
    "            info_table = soup.find(\"table\", {\"class\": re.compile(r\"infobox|roundy\")})\n",
    "            if not info_table:\n",
    "                return None, []\n",
    "            img_tag = info_table.find(\"img\")\n",
    "            image_url = None\n",
    "            if img_tag and img_tag.get(\"src\"):\n",
    "                src = img_tag[\"src\"]\n",
    "                image_url = f\"https:{src}\" if src.startswith(\"//\") else src\n",
    "\n",
    "            type_cells = info_table.find_all(\"a\", href=re.compile(\"/wiki/.*_type\"))\n",
    "            types = [t.get_text(strip=True).lower() for t in type_cells if t.get_text(strip=True)]\n",
    "            return image_url, types or [\"unknown\"]\n",
    "        except Exception:\n",
    "            return None, [\"unknown\"]\n",
    "\n",
    "    def collect_data(self, limit: int = 100):\n",
    "        soup = self.scraper.fetch_page(self.list_url)\n",
    "        tables = soup.find_all(\"table\", {\"class\": re.compile(r\"(roundy|sortable)\")})\n",
    "        count = 0\n",
    "        for table in tables:\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                if count >= limit:\n",
    "                    logging.info(\"Limite atteinte.\")\n",
    "                    return\n",
    "                creature = self.scraper.extract_creature_data(row)\n",
    "                if not creature:\n",
    "                    continue\n",
    "                logging.info(f\"Processing #{creature.index:04d} {creature.name}\")\n",
    "                image_url, types = self.find_creature_image_and_types(creature.url)\n",
    "                if not image_url:\n",
    "                    logging.warning(f\"No image found for {creature.name}\")\n",
    "                    continue\n",
    "                prefix = f\"images/{types[0]}/\"\n",
    "                ext = os.path.splitext(urlparse(image_url).path)[-1] or \".png\"\n",
    "                filename = f\"{creature.index:04d}_{creature.name}{ext}\"\n",
    "                try:\n",
    "                    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "                    image_data = requests.get(image_url, headers=headers).content\n",
    "                    s3_url = self.uploader.upload_image(image_data, filename, prefix=prefix)\n",
    "                    if s3_url:\n",
    "                        logging.info(f\"Uploaded to S3: {s3_url}\")\n",
    "                        count += 1\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to download {image_url}: {e}\")\n",
    "                time.sleep(self.scraper.delay)\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    bucket_name = \"pokemon-scraper-binks\"\n",
    "    collector = DataCollector(bucket_name)\n",
    "    collector.collect_data(limit=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
